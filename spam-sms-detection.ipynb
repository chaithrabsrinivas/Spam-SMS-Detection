{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":982,"sourceType":"datasetVersion","datasetId":483}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Spam SMS Detection: Refined Project\n\nIn this notebook, I build a spam detection model using the SMS Spam Collection dataset.  \nI cover:\n- Data loading and cleaning\n- Exploratory data analysis (EDA)\n- Text preprocessing\n- Building a pipeline with hyperparameter tuning\n- Evaluation with error analysis\n- Basic model interpretation\n\n## Table of Contents\n1. [Installation & Imports](#install)\n2. [Data Loading & Cleaning](#data-loading)\n3. [Exploratory Data Analysis (EDA)](#eda)\n4. [Text Preprocessing](#preprocessing)\n5. [Train-Test Split](#train-test-split)\n6. [Pipeline & Hyperparameter Tuning](#pipeline)\n7. [Evaluation & Error Analysis](#evaluation)\n8. [Model Interpretation](#interpretation)\n9. [Testing on New Messages](#testing)\n10. [Conclusion](#conclusion)\n","metadata":{}},{"cell_type":"code","source":"# ==================================================\n# 1. Installation & Imports\n# ==================================================\n# If needed, I can install missing libraries. On Kaggle, these are usually pre-installed.\n!pip install --upgrade pip\n!pip install joblib eli5\n\nimport pandas as pd\nimport numpy as np\nimport re\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix\nfrom sklearn.pipeline import Pipeline\n\nimport joblib\nimport eli5\n\n# I set the plot style and random seed for reproducibility.\nsns.set(font_scale=1.1)\nplt.style.use('seaborn')  # May show a deprecation warning, but it works fine.\nnp.random.seed(42)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T16:40:21.237097Z","iopub.execute_input":"2025-03-22T16:40:21.237473Z","iopub.status.idle":"2025-03-22T16:40:28.284763Z","shell.execute_reply.started":"2025-03-22T16:40:21.237442Z","shell.execute_reply":"2025-03-22T16:40:28.283621Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id='data-loading'></a>\n## 2. Data Loading & Cleaning\n\nI load the SMS Spam Collection dataset and remove extra columns.\n","metadata":{}},{"cell_type":"code","source":"# I adjust the file path if necessary.\ndata_path = \"/kaggle/input/sms-spam-collection-dataset/spam.csv\"\n\n# I read the CSV using 'latin-1' encoding to handle special characters.\ndf = pd.read_csv(data_path, encoding='latin-1')\n\n# I rename columns for clarity.\ndf.rename(columns={'v1': 'label', 'v2': 'message'}, inplace=True)\n\n# I drop extra columns that contain mostly NaN values.\ncols_to_drop = [col for col in df.columns if 'Unnamed' in col]\ndf.drop(columns=cols_to_drop, inplace=True)\n\nprint(\"=== Data Sample ===\")\ndisplay(df.head())\n\nprint(\"\\n=== Basic Info ===\")\ndf.info()\n\nprint(\"\\n=== Missing Values ===\")\nprint(df.isnull().sum())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T16:40:28.286236Z","iopub.execute_input":"2025-03-22T16:40:28.286515Z","iopub.status.idle":"2025-03-22T16:40:28.321601Z","shell.execute_reply.started":"2025-03-22T16:40:28.28649Z","shell.execute_reply":"2025-03-22T16:40:28.320741Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id='eda'></a>\n## 3. Exploratory Data Analysis (EDA)\n\nI examine the distribution of spam vs. ham messages.\n","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(6,4))\nsns.countplot(x='label', data=df, palette='Set2')\nplt.title('Distribution of Spam vs Ham Messages')\nplt.xlabel('Label')\nplt.ylabel('Count')\nplt.show()\n\nprint(\"Label Counts:\")\nprint(df['label'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T16:40:28.323521Z","iopub.execute_input":"2025-03-22T16:40:28.323749Z","iopub.status.idle":"2025-03-22T16:40:28.473013Z","shell.execute_reply.started":"2025-03-22T16:40:28.32373Z","shell.execute_reply":"2025-03-22T16:40:28.472062Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id='preprocessing'></a>\n## 4. Text Preprocessing\n\nI clean the messages by converting to lowercase, removing punctuation and digits, and stripping extra whitespace.\n","metadata":{}},{"cell_type":"code","source":"def clean_text(text):\n    # Convert to lowercase.\n    text = text.lower()\n    # Remove punctuation and special characters.\n    text = re.sub(r'[^\\w\\s]', '', text)\n    # Remove digits.\n    text = re.sub(r'\\d+', '', text)\n    # Remove extra whitespace.\n    text = text.strip()\n    return text\n\n# I apply the text cleaning function to create a new column.\ndf['cleaned_message'] = df['message'].apply(clean_text)\n\nprint(\"=== Cleaned Sample Messages ===\")\ndisplay(df[['message', 'cleaned_message']].head(5))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T16:40:28.474397Z","iopub.execute_input":"2025-03-22T16:40:28.474677Z","iopub.status.idle":"2025-03-22T16:40:28.519568Z","shell.execute_reply.started":"2025-03-22T16:40:28.474653Z","shell.execute_reply":"2025-03-22T16:40:28.518778Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id='train-test-split'></a>\n## 5. Train-Test Split\n\nI convert labels to numeric and split the data.\n","metadata":{}},{"cell_type":"code","source":"# Map labels to numeric values.\ndf['label_num'] = df['label'].map({'ham': 0, 'spam': 1})\n\n# Define features and target.\nX = df['cleaned_message']\ny = df['label_num']\n\n# I perform an 80/20 train-test split.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(f\"Training set size: {X_train.shape[0]}\")\nprint(f\"Testing set size:  {X_test.shape[0]}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T16:40:28.520478Z","iopub.execute_input":"2025-03-22T16:40:28.520702Z","iopub.status.idle":"2025-03-22T16:40:28.538005Z","shell.execute_reply.started":"2025-03-22T16:40:28.520682Z","shell.execute_reply":"2025-03-22T16:40:28.537147Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id='pipeline'></a>\n## 6. Pipeline & Hyperparameter Tuning\n\nI build a pipeline with TF-IDF vectorizer and Multinomial Na√Øve Bayes. I use GridSearchCV for tuning.\n","metadata":{}},{"cell_type":"code","source":"# I create the pipeline.\npipeline = Pipeline([\n    ('tfidf', TfidfVectorizer(stop_words='english')),\n    ('clf', MultinomialNB())\n])\n\n# I define the parameter grid for GridSearchCV.\nparam_grid = {\n    'tfidf__max_df': [0.5, 0.7, 1.0],\n    'tfidf__ngram_range': [(1, 1), (1, 2)],  # Considering unigrams and bigrams.\n    'clf__alpha': [0.1, 1.0, 5.0]\n}\n\ngrid_search = GridSearchCV(\n    pipeline,\n    param_grid=param_grid,\n    cv=5,\n    n_jobs=-1,\n    verbose=1\n)\n\n# I fit the grid search on the training data.\ngrid_search.fit(X_train, y_train)\n\nprint(\"\\nBest Parameters:\")\nprint(grid_search.best_params_)\nprint(\"\\nBest Cross-Validation Score: {:.3f}\".format(grid_search.best_score_))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T16:40:28.538861Z","iopub.execute_input":"2025-03-22T16:40:28.539139Z","iopub.status.idle":"2025-03-22T16:40:32.619245Z","shell.execute_reply.started":"2025-03-22T16:40:28.539105Z","shell.execute_reply":"2025-03-22T16:40:32.618455Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id='evaluation'></a>\n## 7. Evaluation & Error Analysis\n\nI evaluate the best model on the test set and list misclassified examples.\n","metadata":{}},{"cell_type":"code","source":"# I get the best estimator from grid search.\nbest_model = grid_search.best_estimator_\n\n# I predict on the test set.\ny_pred = best_model.predict(X_test)\n\n# I calculate and print the overall accuracy.\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Test Set Accuracy: {accuracy:.2f}\\n\")\n\n# I print a detailed classification report.\nprint(\"=== Classification Report ===\")\nprint(classification_report(y_test, y_pred))\n\n# I plot the confusion matrix.\ncm = confusion_matrix(y_test, y_pred)\nplt.figure(figsize=(5,4))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=['Ham', 'Spam'], \n            yticklabels=['Ham', 'Spam'])\nplt.title('Confusion Matrix')\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.show()\n\n# Error Analysis: I list some misclassified messages.\nmisclassified = X_test[y_test != y_pred]\nprint(\"=== Misclassified Messages ===\")\ndisplay(misclassified.head(10))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T16:40:32.620127Z","iopub.execute_input":"2025-03-22T16:40:32.620427Z","iopub.status.idle":"2025-03-22T16:40:32.839925Z","shell.execute_reply.started":"2025-03-22T16:40:32.62039Z","shell.execute_reply":"2025-03-22T16:40:32.839073Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id='interpretation'></a>\n## 8. Model Interpretation\n\nI manually extract and display the top features for each class (spam and ham) using the model's log probabilities. The higher the log probability for a feature in a given class, the more indicative it is of that class.\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\n# Get the feature names from the TF-IDF vectorizer.\nfeature_names = best_model.named_steps['tfidf'].get_feature_names_out()\n\n# Get the log probabilities from the MultinomialNB classifier.\n# Shape: (n_classes, n_features)\nlog_prob = best_model.named_steps['clf'].feature_log_prob_\n\ndef print_top_features(class_idx, class_name, top_n=20):\n    top_indices = np.argsort(log_prob[class_idx])[-top_n:][::-1]\n    print(f\"Top features for {class_name}:\")\n    for idx in top_indices:\n        print(f\"{feature_names[idx]}: {log_prob[class_idx][idx]:.4f}\")\n    print(\"\\n\")\n\n# Assuming class 0 corresponds to ham and class 1 to spam.\nprint_top_features(1, \"spam\", top_n=20)\nprint_top_features(0, \"ham\", top_n=20)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T16:42:44.190079Z","iopub.execute_input":"2025-03-22T16:42:44.190417Z","iopub.status.idle":"2025-03-22T16:42:44.22556Z","shell.execute_reply.started":"2025-03-22T16:42:44.190393Z","shell.execute_reply":"2025-03-22T16:42:44.224838Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id='testing'></a>\n## 9. Testing on New Messages\n\nI define a function to predict new messages and test it.\n","metadata":{}},{"cell_type":"code","source":"def predict_message(message):\n    \"\"\"\n    I predict whether a given message is spam or ham.\n    \"\"\"\n    cleaned = clean_text(message)\n    prediction = best_model.predict([cleaned])\n    return 'Spam' if prediction[0] == 1 else 'Ham'\n\n# I test the function with some sample messages.\nsample_messages = [\n    \"Congratulations! You've won a free ticket to the Bahamas. Call now to claim your prize!\",\n    \"Hey, are we still on for dinner tonight?\",\n    \"URGENT! Your mobile number has won $5000. Reply with your bank details.\"\n]\n\nfor msg in sample_messages:\n    print(f\"Message: {msg}\")\n    print(f\"Prediction: {predict_message(msg)}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T16:40:32.885676Z","iopub.execute_input":"2025-03-22T16:40:32.885913Z","iopub.status.idle":"2025-03-22T16:40:32.90398Z","shell.execute_reply.started":"2025-03-22T16:40:32.885892Z","shell.execute_reply":"2025-03-22T16:40:32.903123Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id='conclusion'></a>\n## 10. Conclusion\n\nI built a spam detection model using TF-IDF and Multinomial Na√Øve Bayes with hyperparameter tuning.  \nThis notebook shows the full ML workflow and provides insights into model performance and key features.  \n","metadata":{}}]}